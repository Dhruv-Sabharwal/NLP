{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('war_and_peace_chapter1.txt', sep=\"\\n\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    #wnl = nltk.stem.WordNetLemmatizer()\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "            .encode('ascii', 'ignore')\n",
    "            .decode('utf-8', 'ignore')\n",
    "            .lower())\n",
    "    words = re.sub(r'[^\\w\\s. ]', '', text).split()\n",
    "    return words\n",
    "    #return [wnl.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = basic_clean(' '.join(df[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_grams = pd.Series(nltk.ngrams(words, 1)).value_counts()  # generating bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words=[]\n",
    "for i in range(30):\n",
    "    most_frequent_words.append(one_grams.index[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ngrams(length):\n",
    "    ngrams = {}\n",
    "    length = length\n",
    "\n",
    "    for i in range(len(words)-length):\n",
    "        seq = ' '.join(words[i:i+length])\n",
    "        if seq not in ngrams.keys():\n",
    "            ngrams[seq] = {}\n",
    "        if words[i+length] not in ngrams[seq].keys():\n",
    "            ngrams[seq][words[i+length]] = 1\n",
    "        else:\n",
    "            ngrams[seq][words[i+length]] = ngrams[seq][words[i+length]] + 1\n",
    "    \n",
    "    for i in ngrams.keys():\n",
    "        ngrams[i] = {k: v for k, v in reversed(sorted(ngrams[i].items(), key=lambda item: item[1]))}\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_grams = gen_ngrams(1)\n",
    "three_grams = gen_ngrams(2)\n",
    "four_grams = gen_ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(ngrams, curr_sequence):\n",
    "    high_prob_words = []\n",
    "    keys = ngrams[curr_sequence].keys()\n",
    "    highest_val = ngrams[curr_sequence][list(keys)[0]]\n",
    "    for i in keys:\n",
    "        val = ngrams[curr_sequence][i]\n",
    "        if val < highest_val:\n",
    "            break\n",
    "        high_prob_words.append(i)\n",
    "    return high_prob_words[random.randrange(len(high_prob_words))]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(ngrams, curr_sequence, length, output):\n",
    "    \n",
    "    if len(curr_sequence) < length:\n",
    "        for i in range(length-len(curr_sequence)):\n",
    "            if (nltk.word_tokenize(curr_sequence)[-1]) not in words:\n",
    "                curr_sequence += ' ' + most_frequent_words[random.randrange(len(most_frequent_words))]\n",
    "            else:\n",
    "                curr_sequence += ' ' + generate_next_word(two_grams, curr_sequence)\n",
    "        output = curr_sequence\n",
    "    print(curr_sequence)\n",
    "    while(curr_sequence not in ngrams.keys()):\n",
    "        if (nltk.word_tokenize(curr_sequence)[-1]) not in words:\n",
    "            next_word = most_frequent_words[random.randrange(len(most_frequent_words))]\n",
    "            print(next_word)\n",
    "        else:\n",
    "            next_word = generate_next_word(two_grams, curr_sequence)\n",
    "        output += ' ' + next_word\n",
    "        seq_words = nltk.word_tokenize(output)\n",
    "        curr_sequence = seq_words[-1]\n",
    "        print(curr_sequence)\n",
    "\n",
    "    return curr_sequence, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(ngrams, length, curr_sequence):\n",
    "    output = curr_sequence\n",
    "    for i in range(25):\n",
    "        if curr_sequence not in ngrams.keys():\n",
    "            next_word = most_frequent_words[random.randrange(len(most_frequent_words))]\n",
    "        else:\n",
    "            next_word = generate_next_word(ngrams, curr_sequence)\n",
    "        output += ' ' + next_word\n",
    "        seq_words = nltk.word_tokenize(output)\n",
    "        curr_sequence = ' '.join(seq_words[len(seq_words)-length:len(seq_words)])\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and having got said a not of that that you and not said to anna as to and for by and of not has was has had it\n"
     ]
    }
   ],
   "source": [
    "generate_sentence(four_grams, 3, 'and having got')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_gram_sequence(output):\n",
    "    for i in range(20):\n",
    "        next_word = most_frequent_words[random.randrange(len(most_frequent_words))]\n",
    "        output+=' ' + next_word\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not two minutes had his had not with have in a she be anna that his not and her that not she of'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_one_gram_sequence('not two minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
